\documentclass[preprint]{elsarticle}

\usepackage{amsmath}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}

% Line breaks in URLs
\usepackage{url}
\def\UrlBreaks{\do\/\do-}

% Try to avoid widows and orphans
\clubpenalty = 10000
\widowpenalty = 10000
\displaywidowpenalty = 10000

\begin{document}

\title{Accurate Sampling with Noisy Forces from Approximate Computing}

\begin{abstract}
In scientific computing, the acceleration of atomistic computer simulations by means of custom hardware is finding ever growing application.
A major limitation, however, is that the high efficiency in terms of performance and low power consumption entails the massive usage of low-precision computing units. Here, based on the approximate computing paradigm, we present an algorithmic method to rigorously compensate for numerical inaccuracies due to low-accuracy arithmetic operations, yet still obtaining exact expectation values using a properly modified Langevin-type equation.
\end{abstract}

\begin{keyword}
approximate computing \sep CP2K \sep fluctuation-dissipation theorem \sep FPGA \sep i-PI \sep low precision arithmetic
\end{keyword}

\author[dcm,cs]{Varadarajan Rengaraj}
\ead{rengaraj@campus.uni-paderborn.de}
\author[cs,pc2]{Michael Lass\corref{corauthor}}
\ead{michael.lass@uni-paderborn.de}
\author[cs,pc2]{Christian Plessl}
\ead{christian.plessl@uni-paderborn.de}
\author[dcm,pc2,cssd]{Thomas D. K\"uhne}
\ead{tdkuehne@mail.upb.de}

\cortext[corauthor]{Corresponding author}

\address[dcm]{Dynamics of Condensed Matter, Department of Chemistry, \\ Paderborn University, Warburger Str. 100, 33098 Paderborn, Germany}
\address[cs]{Department of Computer Science, \\ Paderborn University, Warburger Str. 100, 33098 Paderborn, Germany}
\address[pc2]{Paderborn Center for Parallel Computing, \\ Paderborn University, Warburger Str. 100, 33098 Paderborn, Germany}
\address[cssd]{Center for Sustainable Systems Design, \\ Paderborn University, Warburger Str. 100, 33098 Paderborn, Germany}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Molecular dynamics (MD) is a very powerful and widely used technique to study thermodynamic equilibrium properties, as well as the real-time dynamics of complex systems made up of interacting atoms \cite{AlderWainwright1957}. This is done by numerically solving Newton's equations of motion in a time-discretized fashion via computing the nuclear forces of all atoms at every time step \cite{RahmanMD}. Computing these forces by analytically differentiating the interatomic potential with respect to the nuclear coordinates is computationally rather expensive, which is particularly true for electronic structure based \textit{ab-initio} MD simulations \cite{CPMD, CPMD_TDK, PayneRMP, WIRES_TDK}.

For a long time newly developed microchips became faster and more efficient over time due to new manufacturing processes and shrinking transistor sizes. However, this development slowly comes to an end as scaling down the structures of silicon based chips becomes more and more difficult. The focus therefore shifts towards making efficient use of the available technology. Hence, beside algorithmic developments \cite{MTS, Snir, GSE, Shaw, VerletCell, pSHAKE, John, Prodan}, there have been numerous custom computing efforts in this area to increase the efficiency of MD simulations by means of hardware acceleration, which we take up in this work. Examples of the latter are MD implementations on graphics processing units (GPUs) \cite{HOOMD, NAMD, OpenMM, HalMD, Lammps, Amber, Gromacs}, field-programmable gate arrays (FPGAs) \cite{HerbordtI, HerbordtII}, and application-specific integrated circuits (ASICs) \cite{AntonI, AntonII}.
While the use of GPUs for scientific applications is relatively widespread \cite{GPUcomp,Binder,Weigel}, the use of ASICs \cite{QCDScience, QCDOC, GrapeScience, Grape} and FPGAs is less common \cite{JanusI, JanusII, Convey, FDTD, Kenter, Galerkin}, but gained attention over the last years.
In general, to maximize the computational power for a given silicon area, or equivalently minimize the power-consumption per arithmetic operation, more and more computing units are replaced with lower-precision units. This trend is mostly driven by market considerations of the gaming and artificial intelligence industries, which are the target customers of hardware accelerators and naturally do not absolutely rely on full computing accuracy.


In the approach presented in this paper, we make effective use of low-precision special-purpose hardware for general-purpose scientific computing by leveraging the approximate computing (AC) paradigm~\cite{KlavikMalossiBekasEtAl2014, PlesslAC}. The general research goal of AC is to devise and explore ingenious techniques to relax the exactness of a calculation to facilitate the design of more powerful and/or more efficient computer systems. However, in scientific computing, where the exactness of all computed results is of paramount importance, attenuating accuracy requirements is not an option. Yet, we will demonstrate that it is nevertheless possible to rigorously compensate for numerical inaccuracies due to low-accuracy arithmetic operations and still obtain exact expectation values as obtained by ensemble averages of a properly modified Langevin equation.

The remainder of the paper is organized as follows. In Section~\ref{sec:ac} we revisit the basic principles of AC before introducing our modified Langevin equation in Section~\ref{sec:methodology}. Thereafter, in Section~\ref{sec:computational}, we describe the computational details of our computational experiments. Our results are presented and discussed in Section~\ref{sec:results} before concluding the paper in Section~\ref{sec:conclusion}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approximate Computing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ac}

A basic method of approximation and a key requirement for efficient use of processing hardware is the use of adequate data widths in computationally intensive kernels. While in many scientific applications the use of double-precision floating-point is most common, this precision is not always required.
For example, iterative methods can exhibit resilience against low precision arithmetic as has been shown for the computation of inverse matrix roots~\cite{lass17-esl} and for solving systems of linear equations~\cite{KlavikMalossiBekasEtAl2014,Bekas,Dongarra2017,Dongarra2018}.
Mainly driven by the growing popularity of artificial neural networks \cite{Gupta2015}, we can observe growing support of low-precision data types
in hardware accelerators.
In fact, recent GPUs targeting the data center have started supporting half-precision as well, nearly doubling the peak performance compared to single-precision and quadrupling it compared to double-precision arithmetics~\cite{tesla}. However, due to the low number of exponent bits, half-precision only provides a very limited dynamic range. In contrast, \texttt{bfloat16} provides the same dynamic range as single-precision, and just reduces precision. It is currently supported by Google's Tensor Processing Units (TPU)~\cite{tpu} and support is announced for future Intel Xeon processors~\cite{xeon} and Intel AgileX FPGAs. A list of commonly used data types, together with the corresponding number of bits used to store the exponent and the mantissa, are shown in Table~\ref{tab:float}.
\begin{table}
  \caption{Bitwidth of common floating-point formats}
  \centering
  \label{tab:float}
  \begin{tabular}{lrrr}
    Type & sign & exponent & mantissa \\
    \hline
    IEEE 754 Quadruple-precision & 1 & 15 & 112 \\
    IEEE 754 Double-precision & 1 & 11 & 52 \\
    IEEE 754 Single-precision & 1 & 8 & 23 \\
    IEEE 754 Half-precision & 1 & 5 & 10 \\
    Bfloat16 (truncated IEEE single-precision) & 1 & 8 & 7
  \end{tabular}
\end{table}

Yet, programmable hardware such as FPGAs, as a platform for custom-built accelerator designs \cite{Strzodka2006, KenterVector, KenterPragma}, can make effective use of all of these, but also entirely custom number formats.
Developers can specify the number of exponent and mantissa bits and trade off precision against the amount of memory blocks required to store values and the number of logic elements required to perform arithmetic operations on them.

In addition to floating-point formats, also fixed-point representations can be used. Here, all numbers are stored as integers of fixed size with a
predefined scaling factor. Calculations are thereby performed using integer arithmetic. On CPUs and GPUs only certain models can perform integer operations with a peak performance similar to that of floating-point arithmetic, depending on the capabilities of the vector units / stream processors. Nevertheless, FPGAs typically can perform integer operations with performance similar to or even higher than that of floating-point. Due to the high flexibility of FPGAs with respect to different data formats and the possible use of entirely custom data types, we see them as the main target technology for our work. For this reason, we consider both floating-point and fixed-point arithmetic in the following.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:methodology}
To demonstrate the concept of approximate computing, we introduce white noise to the interatomic forces that are computed while running the MD simulation. In this section, we describe in detail on how we introduce the noise to mimic in software the behaviour that would be observed when running the MD on the actual FPGA or GPU hardware with reduced numerical precision. We classify the computational errors into two types: fixed-point errors, and floating-point errors. Assuming that $\textbf{F}_{I}$ are the exact and $\textbf{F}_{I}^{N}$ the noisy forces from a MD simulations with low precision on an FPGA for instance, fixed-point errors can by modelled by

\begin{equation}
\textbf{F}_{I}^{N}=
\begin{pmatrix}
\text{F}_{I}^{x}\\
\text{F}_{I}^{y}\\
\text{F}_{I}^{z}\\
\end{pmatrix} +
\begin{pmatrix}
c_{1} \times 10^{-\beta }\\
c_{2} \times 10^{-\beta }\\
c_{3} \times 10^{-\beta }\\
\end{pmatrix}
,
\end{equation}
whereas floating-point errors are described by
\begin{equation}
\textbf{F}_{I}^{N} =
\begin{pmatrix}
\text{F}_{I}^{x} \times 10^{-\alpha_1}\\
\text{F}_{I}^{y} \times 10^{-\alpha_2}\\
\text{F}_{I}^{z} \times 10^{-\alpha_3}\\
\end{pmatrix} +
\begin{pmatrix}
c_{1} \times 10^{-(\alpha_1+\beta)}\\
c_{2} \times 10^{-(\alpha_2+\beta)}\\
c_{3} \times 10^{-(\alpha_3+\beta)}\\
\end{pmatrix}
.
\end{equation}
Therein, $c_1$, $c_2$ and $c_3$ are random values chosen in the range [-0.5, 0.5], whereas $\text{F}_{I}^{x}$, $\text{F}_{I}^{y}$ and $\text{F}_{I}^{x}$ are the individual force components of $\textbf{F}_{I}$, respectively. The floating-point scaling factor is denoted as $\alpha$ and the magnitude of the applied noise by \(\beta\).

To rigorously correct the errors introduced by numerical noise we employ a modified Langevin equation. In particular, we model the force as obtained by a low-precision computation on a GPU or FPGA-based accelerator as
\begin{equation} \label{fFPGA}
\textbf{F}_{I}^{N} = \textbf{F}_{I} + \mathbf{\Xi }_{I}^{N},
\end{equation}
where $\mathbf{\Xi }_{I}^{N}$ is an additive white noise for which
\begin{equation} \label{CrossCorr}
 \left \langle \textbf{F}_{I}\left ( 0 \right ) \mathbf{\Xi } _{I}^{N}\left ( t \right )\right \rangle \cong  0
\end{equation}
holds. Throughout, $\langle \cdots \rangle$ denotes Boltzmann-weighted ensemble averages as obtained by the partition function $Z=\text{Tr} \exp(-E/k_B T)$, where $E$ is the potential energy, $k_B$ the so-called Boltzmann constant, and $T$ the temperature. Given that $\mathbf{\Xi }_{I}^{N}$ is unbiased, which in our case is true by its very definition, it is nevertheless possible to accurately sample the Boltzmann distribution by means of a Langevin-type equation \cite{Krajewski,Richters,Karhan}, which in its general form reads as
\begin{equation} \label{LangevinEq}
M_{I}\ddot{\textbf{R}}_{I}=\textbf{F}_{I}+\mathbf{\Xi }_{I}^{N}-\gamma _{N}M_{I}\dot{\textbf{R}}_{I},
\end{equation}
where $\dot{\textbf{R}}_{I}$ are the nuclear coordinates (the dot denotes time derivative), $M_I$ are the nuclear masses and $\gamma _{N}$ is a damping coefficient,
which is chosen to compensate for \(\mathbf{\Xi }_{I}^{N}\). The latter, in order to guarantee an accurate canonical sampling, has to obey
the fluctuation-dissipation theorem
\begin{equation}
\left \langle \mathbf{\Xi }_{I}^{N}\left ( 0 \right ) \mathbf{\Xi }_{I}^{N}\left ( t \right ) \right \rangle \cong  2 \gamma_{N} M_I k_{B} T  \delta \left ( t \right ).
\label{FDT}
\end{equation}

Substituting Eq.~\ref{fFPGA} into Eq.~\ref{LangevinEq} results in the desired modified Langevin equation
\begin{equation} \label{modLangevin}
M_{I}\ddot{\textbf{R}}_{I} = \textbf{F}_{I}^{N}-\gamma _{N}M_{I}\dot{\textbf{R}}_{I},
\end{equation}
which will be used throughout the remaining of this paper. This is to say that the noise, as originating from a low-precision computation, can be thought of as the additive white noise of a thitherto unknown damping coefficient $\gamma_N$, which satisfies the fluctuation-dissipation theorem of Eq.~\ref{FDT}.  The specific value of $\gamma_N$ is determined in such a way so as to generate the correct average temperature, as measured by the equipartition theorem
\begin{equation}
\left\langle \frac{1}{2} M_I \dot{\textbf{R}}_{I} \right\rangle = \frac{3}{2} k_B T.
\label{EquiPartTheorem}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computational details}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:computational}
To demonstrate our approach we have implemented it in the CP2K suite of programs \cite{cp2k}. More precisely, we have conducted MD simulations of liquid Silicon (Si) at 3000~K using the environment-dependent interatomic potential of Bazant et al. \cite{EIP1,EIP2}.
All simulations consisted of 1000 Si atoms in a 3D-periodic cubic box of length 27.155~\AA. Using the algorithm of Ricci and Ciccotti \cite{Ricci}, Eq.~\ref{LangevinEq} was integrated with a discretized timestep of 1.0~fs with $\gamma_N = 0.001~$fs$^{-1}$.

Whereas the latter settings were used to compute our reference data, in total six different cases of fixed-point and floating-point errors were investigated by varying the exponent $\beta$ between 0 (huge noise) and 3 (tiny noise) that is, ranging from $1/1000$ of the physical force up to the same magnitude as the force.
As already alluded to above, the additive white noise is compensated by means of Eq.~\ref{modLangevin} by varying $\gamma_N$ on-the-fly using a Berendsen-like algorithm until the equipartition theorem of Eq.~\ref{EquiPartTheorem} is satisfied \cite{Berendsen,TDKwater,TDKrev}. Alternatively, $\gamma_N$ can be computed by integrating the autocorrelation function of the additive white noise \cite{RZK}.
In Table~\ref{tab:gamma} the resulting values of \textit{\(\gamma_N^{fix}\)} for fixed-point and \textit{\(\gamma_N^{float}\)} for floating-point errors are reported as a function of \textit{\(\beta\)}.
\begin{table}
  \caption{Values for \textit{\(\gamma_N^{fix}\)} and \textit{\(\gamma_N^{float}\)} as a function of \textit{\(\beta\)}.}
  \centering
  \label{tab:gamma}
  \begin{tabular}{lrr}
    \textit{\(\beta\)} & \textit{\(\gamma_N^{fix}\)} & \textit{\(\gamma_N^{float}\)} \\
    \hline
    0 &           & 0.00025  \\
    1 & 0.0004    & 0.000005 \\
    2 & 0.000009  & 0.000005 \\
    3 & 0.0000009 &
  \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:results}
As can be directly deduced from Table~\ref{tab:gamma}, the smaller values of $\gamma_N$ for a given $\beta$ immediately suggest the higher noise resilience when using floating-point as compared to fixed-point numbers.
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]
{Fixed_point_rdf2.pdf}
\end{center}
\caption{\label{Fig1}
Partial pair correlation function for liquid Si at 3000~K with noisy forces introduced by fixed-point errors of magnitude $10^{-3}$ (blue), $10^{-2}$ (green) and $10^{-1}$ (red). For comparison, the results, as obtained by our reference calculations without noise, are shown in black.
} \end{figure}
In Figs.~\ref{Fig1} and \ref{Fig2}, the Si-Si partial pair-correlation function $g(r)$, which describes how the particle-density varies as a function of distance from a reference particle (atoms, molecules, colloids, etc.), as computed using an optimal scheme for orthorombic systems \cite{KAF}, is shown for different values of $\beta$.
As can be seen, for both fixed-point and floating-point errors, the agreement with our reference calculation is nearly perfect up to the highest noise we have investigated. As already anticipated earlier, the usage of floating-point errors is not only able to tolerate higher noise levels, but is also throughout more accurate.
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]
{Floating_point_rdf2.pdf}
\end{center}
\caption{\label{Fig2}
Partial pair correlation function for liquid Si at 3000~K with noisy forces introduced by floating-point errors of magnitude $10^{-2}$ (blue), $10^{-1}$ (green) and $10^{-0}$ (red). For comparison, the results, as obtained by our reference calculations without noise, are shown in black.
} \end{figure}

To verify that the sampling is indeed canonical, in Fig.~\ref{Fig3} the actual kinetic energy distribution as obtained by our simulations using noisy forces is depicted and compared to the analytic Maxwell distribution. It is evident that if sampled long enough, not only the mean value, but also the distribution tails are in excellent agreement with the exact Maxwellian kinetic energy distribution, which demonstrates that we are indeed performing a canonical sampling.
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]
{maxwelldistribution_new.pdf}
\end{center}
\caption{\label{Fig3}
Kinetic energy distribution of liquid Si at 3000~K, as obtained by our simulations using noisy forces (circles). For comparison the analytic Maxwell distribution is also shown (line).
} \end{figure}
To further assess the accuracy of the present method, we expand the autocorrelation of the noisy forces, i.e.
\begin{subequations}
\begin{eqnarray}
  && \left \langle \textbf{F}_{I}^{N}\left ( 0 \right )\textbf{F}_{I}^{N}\left ( t \right )\right \rangle \\
  &=& \left \langle \left ( \textbf{F}_{I}\left ( 0 \right ) + \mathbf{\Xi } _{I}^{N} \left(0 \right )\right) \left( \textbf{F}_{I}\left ( t \right )+\mathbf{\Xi } _{I}^{N}\left ( t \right )\right) \right \rangle \\
  &=& \left \langle \textbf{F}_{I}\left ( 0 \right ) \textbf{F}_{I}\left ( t \right )\right \rangle + \left \langle \textbf{F}_{I}\left ( 0 \right ) \mathbf{\Xi } _{I}^{N}\left(t \right )\right \rangle \label{AutoCorr} \\
  &+& \left \langle \textbf{F}_{I}\left ( t \right ) \mathbf{\Xi } _{I}^{N}\left(0 \right )\right \rangle + \left \langle \mathbf{\Xi } _{I}^{N}\left(0 \right ) \mathbf{\Xi } _{I}^{N}\left(t \right )\right \rangle.  \nonumber
\end{eqnarray}
\end{subequations}
Since the cross correlation terms between the exact force and the additive white noise is vanishing due to Eq.~\ref{CrossCorr}, comparing the autocorrelation of the noisy forces $\langle \textbf{F}_{I}^{N}(0)\textbf{F}_{I}^{N}(t)\rangle$ with the autocorrelation of the exact forces $\langle \textbf{F}_{I}(0) \textbf{F}_{I}(t)\rangle$ permits to assess the localization of the last term of Eq.~\ref{AutoCorr}.
The fact that $\langle \textbf{F}_{I}^{N}(0)\textbf{F}_{I}^{N}(t)\rangle$ is essentially identical to $\langle \textbf{F}_{I}(0) \textbf{F}_{I}(t)\rangle$, as can be seen in Fig.~\ref{Fig4}, implies that $\langle \mathbf{\Xi } _{I}^{N}(0) \mathbf{\Xi } _{I}^{N}(t)\rangle$ is very close to a $\delta$-function as required by the fluctuation-dissipation theorem in order to ensure an accurate canonical sampling of the Boltzmann distribution. In other words, from this it follows that our initial assumption underlying Eq.~\ref{modLangevin}, to model the noise due to a low-precision calculation as an additive white noise channel, is justified.
\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]
{AutocorrelationPlot_n.pdf}
\end{center}
\caption{\label{Fig4}
The Autocorrelation of the noisy forces \(
\left \langle \textbf{F}_{I}^{N}\left ( 0 \right ) \textbf{F}_{I}^{N}\left ( t \right )\right \rangle \)(line), which are compared to the autocorrelation of the exact forces \( \left \langle \textbf{F}_{I}\left ( 0 \right ) \textbf{F}_{I}\left ( t \right )\right \rangle \)(circles).
} \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:conclusion}
We conclude by noting that the present method has been recently implemented in the universal force engine i-PI \cite{iPi}, which can be generally applied to all sorts of forces affected by stochastic noise such as those computed by GPUs or other hardware accelerators~\cite{HOOMD, NAMD, OpenMM, HalMD, Lammps, Amber, Gromacs}, and potentially even quantum computing devices \cite{Steane, Knill, Blatt, Chow}. The possibility to apply similar ideas to N-body simulations~\cite{White, Makino} and to combine it with further algorithmic approximations~\cite{LassAC} is to be underlined and will be presented elsewhere.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The authors would like to thank the Paderborn Center for Parallel Computing (PC2) for computing time on OCuLUS and FPGA-based supercomputer NOCTUA. Funding from the Paderborn University's research award for ``Green IT'' is kindly acknowledged. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Grant Agreement No.:~716142).

\bibliographystyle{elsarticle-num}
\biboptions{sort&compress}
\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{AlderWainwright1957}
B.~J. Alder, T.~E. Wainwright, Phase transition for a hard sphere system, J.
  Chem. Phys. 27~(5) (1957) 1208--1209.
\newblock \href {https://doi.org/10.1063/1.1743957}
  {\path{doi:10.1063/1.1743957}}.

\bibitem{RahmanMD}
A.~Rahman, Correlations in the motion of atoms in liquid argon, Phys. Rev.
  136~(2A) (1964) A405--A411.
\newblock \href {https://doi.org/10.1103/PhysRev.136.A405}
  {\path{doi:10.1103/PhysRev.136.A405}}.

\bibitem{CPMD}
R.~Car, M.~Parrinello, Unified approach for molecular dynamics and
  density-functional theory, Phys. Rev. Lett. 55~(22) (1985) 2471--2474.
\newblock \href {https://doi.org/10.1103/PhysRevLett.55.2471}
  {\path{doi:10.1103/PhysRevLett.55.2471}}.

\bibitem{CPMD_TDK}
T.~D. K\"uhne, M.~Krack, F.~R. Mohamed, M.~Parrinello, Efficient and accurate
  car-parrinello-like approach to born-oppenheimer molecular dynamics, Phys.
  Rev. Lett. 98~(6) (2007) 066401.
\newblock \href {https://doi.org/10.1103/PhysRevLett.98.066401}
  {\path{doi:10.1103/PhysRevLett.98.066401}}.

\bibitem{PayneRMP}
M.~C. Payne, M.~P. Teter, D.~C. Allan, T.~A. Arias, J.~D. Joannopoulos,
  Iterative minimization techniques for ab initio total-energy calculations:
  molecular dynamics and conjugate gradients, Rev. Mod. Phys. 64~(4) (1992)
  1045--1097.
\newblock \href {https://doi.org/10.1103/RevModPhys.64.1045}
  {\path{doi:10.1103/RevModPhys.64.1045}}.

\bibitem{WIRES_TDK}
T.~D. K\"uhne, Second generation car–parrinello molecular dynamics, WIREs
  Comput. Mol. Sci. 4~(4) (2014) 391--406.
\newblock \href {https://doi.org/10.1002/wcms.1176}
  {\path{doi:10.1002/wcms.1176}}.

\bibitem{MTS}
M.~E. Tuckerman, B.~J. Berne, G.~J. Martyna, Reversible multiple time scale
  molecular dynamics, J. Chem. Phys. 97~(3) (1992) 1990--2001.
\newblock \href {https://doi.org/10.1063/1.463137}
  {\path{doi:10.1063/1.463137}}.

\bibitem{Snir}
M.~Snir, A note on n-body computations with cutoffs, Theor. Comput. Syst.
  37~(2) (2004) 295--318.
\newblock \href {https://doi.org/10.1007/s00224-003-1071-0}
  {\path{doi:10.1007/s00224-003-1071-0}}.

\bibitem{GSE}
Y.~Shan, J.~L. Klepeis, M.~P. Eastwood, R.~O. Dror, D.~E. Shaw, Gaussian split
  ewald: A fast ewald mesh method for molecular simulation, J. Chem. Phys.
  122~(5) (2005) 054101.
\newblock \href {https://doi.org/10.1063/1.1839571}
  {\path{doi:10.1063/1.1839571}}.

\bibitem{Shaw}
D.~E. Shaw, A fast, scalable method for the parallel evaluation of
  distance-limited pairwise particle interactions, J. Comput. Chem. 26~(13)
  (2005) 1318--1328.
\newblock \href {https://doi.org/10.1002/jcc.20267}
  {\path{doi:10.1002/jcc.20267}}.

\bibitem{VerletCell}
P.~Gonnet, Pairwise verlet lists: Combining cell lists and verlet lists to
  improve memory locality and parallelism, J. Comp. Chem. 33~(1) (2012) 76--81.
\newblock \href {https://doi.org/10.1002/jcc.21945}
  {\path{doi:10.1002/jcc.21945}}.

\bibitem{pSHAKE}
P.~Gonnet, A quadratically convergent shake in o(n(2)), J. Comp. Phys. 220~(2)
  (2007) 740--750.
\newblock \href {https://doi.org/10.1016/j.jcp.2006.05.032}
  {\path{doi:10.1016/j.jcp.2006.05.032}}.

\bibitem{John}
C.~John, T.~Spura, S.~Habershon, T.~D. K\"uhne, Quantum ring-polymer
  contraction method: Including nuclear quantum effects at no additional
  computational cost in comparison to ab initio molecular dynamics, Phys. Rev.
  E 93~(4) (2016) 043305.
\newblock \href {https://doi.org/10.1103/PhysRevE.93.043305}
  {\path{doi:10.1103/PhysRevE.93.043305}}.

\bibitem{Prodan}
T.~D. K\"uhne, E.~Prodan, Disordered crystals from first principles i:
  Quantifying the configuration space, Annals of Physics 391~(1) (2018)
  120--149.
\newblock \href {https://doi.org/10.1016/j.aop.2018.01.016}
  {\path{doi:10.1016/j.aop.2018.01.016}}.

\bibitem{HOOMD}
J.~A. Anderson, C.~D. Lorenz, A.~Travesset, General purpose molecular dynamics
  simulations fully implemented on graphics processing units, J. Comp. Phys.
  227~(10) (2008) 5342--5359.
\newblock \href {https://doi.org/10.1016/j.jcp.2008.01.047}
  {\path{doi:10.1016/j.jcp.2008.01.047}}.

\bibitem{NAMD}
J.~E. Stone, D.~J. Hardy, I.~S. Ufimtsev, K.~Schulten, Gpu-accelerated
  molecular modeling coming of age, J. Mol. Graphics Modell. 29~(2) (2010)
  116--125.
\newblock \href {https://doi.org/10.1016/j.jmgm.2010.06.010}
  {\path{doi:10.1016/j.jmgm.2010.06.010}}.

\bibitem{OpenMM}
P.~Eastman, V.~S. Pande, Openmm: A hardware-independent framework for molecular
  simulations, Comput. Sci. Eng. 12 (2010) 34--39.
\newblock \href {https://doi.org/10.1109/MCSE.2010.27}
  {\path{doi:10.1109/MCSE.2010.27}}.

\bibitem{HalMD}
P.~H. Colberg, F.~H\"ofling, Highly accelerated simulations of glassy dynamics
  using gpus: Caveats on limited floating-point precision, Comp. Phys. Commun.
  182~(5) (2011) 1120--1129.
\newblock \href {https://doi.org/10.1016/j.cpc.2011.01.009}
  {\path{doi:10.1016/j.cpc.2011.01.009}}.

\bibitem{Lammps}
W.~M. Brown, A.~Kohlmeyer, S.~J. Plimpton, A.~N. Tharrington, Implementing
  molecular dynamics on hybrid high performance computers –
  particle–particle particle-mesh, Comp. Phys. Commun. 183~(3) (2012)
  449--459.
\newblock \href {https://doi.org/10.1016/j.cpc.2011.10.012}
  {\path{doi:10.1016/j.cpc.2011.10.012}}.

\bibitem{Amber}
S.~{Le Grand}, A.~W. G\"otz, R.~C. Walker, Spfp: Speed without compromise—a
  mixed precision model for gpu accelerated molecular dynamics simulations,
  Comp. Phys. Commun. 184~(2) (2013) 374--380.
\newblock \href {https://doi.org/10.1016/j.cpc.2012.09.022}
  {\path{doi:10.1016/j.cpc.2012.09.022}}.

\bibitem{Gromacs}
A.~J. Abraham, T.~Murtola, R.~Schulz, S.~Pall, J.~C. Smith, B.~Hess,
  E.~Lindahl, Gromacs: High performance molecular simulations through
  multi-level parallelism from laptops to supercomputers, SoftwareX 1--2 (2015)
  19--25.
\newblock \href {https://doi.org/10.1016/j.softx.2015.06.001}
  {\path{doi:10.1016/j.softx.2015.06.001}}.

\bibitem{HerbordtI}
M.~C. Herbordt, Y.~Gu, T.~VanCourt, J.~Model, B.~Sukhwani, M.~Chiu, Computing
  models for fpga-based accelerators, Comput. Sci. Eng. 10~(6) (2008) 35--45.
\newblock \href {https://doi.org/10.1109/MCSE.2008.143}
  {\path{doi:10.1109/MCSE.2008.143}}.

\bibitem{HerbordtII}
M.~C. Herbordt, Y.~Gu, T.~VanCourt, J.~Model, B.~Sukhwani, M.~Chiu, Explicit
  design of fpga-based coprocessors for short-range force computations in
  molecular dynamics simulations, Parallel Computing 34~(4--5) (2008) 261--277.
\newblock \href {https://doi.org/10.1016/j.parco.2008.01.007}
  {\path{doi:10.1016/j.parco.2008.01.007}}.

\bibitem{AntonI}
D.~E. Shaw, M.~M. Deneroff, R.~O. Dror, J.~S. Kuskin, R.~H. Larson, J.~K.
  Salmon, C.~Young, B.~Batson, K.~J. Bowers, J.~C. Chao, M.~P. Eastwood,
  J.~Gagliardo, J.~P. Grossman, C.~R. Ho, D.~J. Ierardi, I.~Kolossv\'{a}ry,
  J.~L. Klepeis, T.~Layman, C.~McLeavey, M.~A. Moraes, R.~Mueller, E.~C.
  Priest, Y.~Shan, J.~Spengler, M.~Theobald, B.~Towles, S.~C. Wang, Anton, a
  special-purpose machine for molecular dynamics simulation, in: Proceedings of
  the 34th Annual International Symposium on Computer Architecture, ISCA '07,
  ACM, New York, NY, USA, 2007, pp. 1--12.
\newblock \href {https://doi.org/10.1145/1250662.1250664}
  {\path{doi:10.1145/1250662.1250664}}.

\bibitem{AntonII}
D.~E. Shaw, J.~P. Grossman, J.~A. Bank, B.~Batson, J.~A. Butts, J.~C. Chao,
  M.~M. Deneroff, R.~O. Dror, A.~Even, C.~H. Fenton, A.~Forte, J.~Gagliardo,
  G.~Gill, B.~Greskamp, C.~R. Ho, D.~J. Ierardi, L.~Iserovich, J.~S. Kuskin,
  R.~H. Larson, T.~Layman, L.-S. Lee, A.~K. Lerer, C.~Li, D.~Killebrew, K.~M.
  Mackenzie, S.~Y.-H. Mok, M.~A. Moraes, R.~Mueller, L.~J. Nociolo, J.~L.
  Peticolas, T.~Quan, D.~Ramot, J.~K. Salmon, D.~P. Scarpazza, U.~Ben~Schafer,
  N.~Siddique, C.~W. Snyder, J.~Spengler, P.~T.~P. Tang, M.~Theobald, H.~Toma,
  B.~Towles, B.~Vitale, S.~C. Wang, C.~Young, Anton 2: Raising the bar for
  performance and programmability in a special-purpose molecular dynamics
  supercomputer, in: Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis, SC '14, IEEE Press,
  Piscataway, NJ, USA, 2014, pp. 41--53.
\newblock \href {https://doi.org/10.1109/SC.2014.9}
  {\path{doi:10.1109/SC.2014.9}}.

\bibitem{GPUcomp}
J.~D. {Owens}, M.~{Houston}, D.~{Luebke}, S.~{Green}, J.~E. {Stone}, J.~C.
  {Phillips}, Gpu computing, Proceedings of the IEEE 96~(5) (2008) 879--899.
\newblock \href {https://doi.org/10.1109/JPROC.2008.917757}
  {\path{doi:10.1109/JPROC.2008.917757}}.

\bibitem{Binder}
T.~Preis, P.~Virnau, W.~Paul, J.~J. Schneider, Gpu accelerated monte carlo
  simulation of the 2d and 3d ising model, J. Comp. Phys. 228~(12) (2009)
  4468--4477.
\newblock \href {https://doi.org/10.1016/j.jcp.2009.03.018}
  {\path{doi:10.1016/j.jcp.2009.03.018}}.

\bibitem{Weigel}
M.~Weigel, Performance potential for simulating spin models on gpu, J. Comp.
  Phys. 231~(8) (2012) 3064--3082.
\newblock \href {https://doi.org/10.1016/j.jcp.2011.12.008}
  {\path{doi:10.1016/j.jcp.2011.12.008}}.

\bibitem{QCDScience}
F.~R. Brown, N.~H. Christ, Parallel supercomputers for lattice gauge theory,
  Science 239~(4846) (1988) 1393--1400.
\newblock \href {https://doi.org/10.1126/science.239.4846.1393}
  {\path{doi:10.1126/science.239.4846.1393}}.

\bibitem{QCDOC}
P.~A. {Boyle}, D.~{Chen}, N.~H. {Christ}, M.~A. {Clark}, S.~D. {Cohen},
  C.~{Cristian}, Z.~{Dong}, A.~{Gara}, B.~{Joo}, C.~{Jung}, C.~{Kim}, L.~A.
  {Levkova}, X.~{Liao}, G.~{Liu}, R.~D. {Mawhinney}, S.~{Ohta}, K.~{Petrov},
  T.~{Wettig}, A.~{Yamaguchi}, Overview of the qcdsp and qcdoc computers, IBM
  Journal of Research and Development 49~(2.3) (2005) 351--365.
\newblock \href {https://doi.org/10.1147/rd.492.0351}
  {\path{doi:10.1147/rd.492.0351}}.

\bibitem{GrapeScience}
P.~Hut, J.~Makino, Astrophysics on the grape family of special-purpose
  computers, Science 283~(5401) (1999) 501--505.
\newblock \href {https://doi.org/10.1126/science.283.5401.501}
  {\path{doi:10.1126/science.283.5401.501}}.

\bibitem{Grape}
T.~Fukushige, P.~Hut, J.~Makino, High-performance special-purpose computers in
  science, Comput. Sci. Eng. 1~(2) (1999) 12--13.
\newblock \href {https://doi.org/10.1109/5992.753041}
  {\path{doi:10.1109/5992.753041}}.

\bibitem{JanusI}
F.~Belletti, M.~Cotallo, A.~Cruz, L.~A. Fernandez, A.~Gordillo-Guerrero,
  M.~Guidetti, A.~Maiorano, F.~Mantovani, E.~Marinari, V.~Martin-Mayor,
  A.~Munoz-Sudupe, D.~Navarro, G.~Parisi, S.~Perez-Gaviro, M.~Rossi, J.~J.
  Ruiz-Lorenzo, S.~F. Schifano, D.~Sciretti, A.~Tarancon, R.~Tripiccione,
  Janus: An fpga-based system for high-performance scientific computing,
  Comput. Sci. Eng. 11~(1) (2009) 48.
\newblock \href {https://doi.org/10.1109/MCSE.2009.11}
  {\path{doi:10.1109/MCSE.2009.11}}.

\bibitem{JanusII}
M.~Baity-Jesi, R.~A. R.A.~Banos, A.~Cruz, L.~A. Fernandez, J.~M. Gil-Narvion,
  A.~Gordillo-Guerrero, D.~Iniguez, A.~Maiorano, F.~Mantovani, E.~Marinari,
  V.~Martin-Mayor, J.~Monforte-Garcia, A.~Munoz-Sudupe, D.~Navarro, G.~Parisi,
  S.~Perez-Gaviro, M.~Pivanti, F.~Ricci-Tersenghi, J.~J. Ruiz-Lorenzo, S.~F.
  Schifano, B.~Seoane, A.~Tarancon, R.~Tripiccione, D.~Yllanes, Janus ii: A new
  generation application-driven computer for spin-system simulations, Comp.
  Phys. Commun. 185~(2) (2014) 550--559.
\newblock \href {https://doi.org/10.1016/j.cpc.2013.10.019}
  {\path{doi:10.1016/j.cpc.2013.10.019}}.

\bibitem{Convey}
B.~Meyer, J.~Schumacher, C.~Plessl, J.~Forstner, Convey vector personalities -
  fpga acceleration with an openmp-like programming effort?, in: Proc. Int.
  Conf. on Field Programmable Logic and Applications (FPL), FPL '12, IEEE,
  2012, pp. 189--196.
\newblock \href {https://doi.org/10.1109/FPL.2012.6339259}
  {\path{doi:10.1109/FPL.2012.6339259}}.

\bibitem{FDTD}
H.~Giefers, C.~Plessl, J.~F\"{o}rstner, Accelerating finite difference time
  domain simulations with reconfigurable dataflow computers, SIGARCH Comput.
  Archit. News 41~(5) (2014) 65--70.
\newblock \href {https://doi.org/10.1145/2641361.2641372}
  {\path{doi:10.1145/2641361.2641372}}.

\bibitem{Kenter}
T.~Kenter, J.~F\"orstner, C.~Plessl, Flexible fpga design for fdtd using
  opencl, in: Proc. Int. Conf. on Field Programmable Logic and Applications,
  FPL '17, IEEE, 2017.
\newblock \href {https://doi.org/10.23919/FPL.2017.8056844}
  {\path{doi:10.23919/FPL.2017.8056844}}.

\bibitem{Galerkin}
T.~Kenter, G.~Mahale, S.~Alhaddad, Y.~Grynko, C.~Schmitt, A.~Afzal, F.~Hannig,
  J.~F\"orstner, C.~Plessl, Opencl-based fpga design to accelerate the nodal
  discontinuous galerkin method for unstructured meshes, in: Pro. Int. Symp. on
  Field-Programmable Custom Computing Machines, Vol.~1 of FCCM '18, 2018, pp.
  189--196.
\newblock \href {https://doi.org/10.1109/FCCM.2018.00037}
  {\path{doi:10.1109/FCCM.2018.00037}}.

\bibitem{KlavikMalossiBekasEtAl2014}
P.~Klav{\'\i}k, A.~C.~I. Malossi, C.~Bekas, A.~Curioni, {Changing Computing
  Paradigms Towards Power Efficiency}, Philosophical Transactions of the Royal
  Society {A}: Mathematical, Physical \& Engineering Sciences 372~(2018)
  (2014).

\bibitem{PlesslAC}
C.~Plessl, M.~Platzner, P.~J. Schreier, Approximate computing, Informatik
  Spektrum 38~(5) (2015) 396--399.
\newblock \href {https://doi.org/10.1007/s00287-015-0911-z}
  {\path{doi:10.1007/s00287-015-0911-z}}.

\bibitem{lass17-esl}
M.~Lass, T.~D. K\"uhne, C.~Plessl, Using approximate computing for the
  calculation of inverse matrix p-th roots, IEEE Embedded Systems Letters
  10~(2) (2018) 33--36.

\bibitem{Bekas}
C.~M. Angerer, R.~Polig, D.~Zegarac, H.~Giefers, C.~Hagleitner, C.~Bekas,
  A.~Curioni, A fast, hybrid, power-efficient high-precision solver for large
  linear systems based on low-precision hardware 12 (2016) 72--82.
\newblock \href {https://doi.org/10.1016/j.suscom.2015.10.001}
  {\path{doi:10.1016/j.suscom.2015.10.001}}.

\bibitem{Dongarra2017}
A.~Haidar, P.~Wu, S.~Tomov, J.~Dongarra, Investigating half precision
  arithmetic to accelerate dense linear system solvers, in: 8th Workshop on
  Latest Advances in Scalable Algorithms for Large-Scale Systems, ScalA17, ACM,
  2017.
\newblock \href {https://doi.org/10.1145/3148226.3148237}
  {\path{doi:10.1145/3148226.3148237}}.

\bibitem{Dongarra2018}
A.~Haidar, S.~Tomov, J.~Dongarra, N.~J. Higham, Harnessing gpu tensor cores for
  fast fp16 arithmetic to speed up mixed-precision iterative refinement
  solvers, in: Proceedings of the International Conference for High Performance
  Computing, Networking, Storage, and Analysis, SC '18, IEEE, 2018.
\newblock \href {https://doi.org/10.1109/SC.2018.00050}
  {\path{doi:10.1109/SC.2018.00050}}.

\bibitem{Gupta2015}
S.~Gupta, A.~Agrawal, K.~Gopalakrishnan, P.~Narayanan, Deep learning with
  limited numerical precision, in: Proceedings of the 32nd International
  Conference on International Conference on Machine Learning, ICML'15, ACM,
  2015, pp. 1737--1746.

\bibitem{tesla}
{NVIDIA Corporation}, Tesla {P100} data sheet (Oct. 2016).

\bibitem{tpu}
{The Next Platform},
  \href{https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/}{Tearing
  apart {Google's} {TPU} 3.0 {AI} coprocessor}, Online Article (May 2018).
\newline\urlprefix\url{https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/}

\bibitem{xeon}
{Top 500},
  \href{https://www.top500.org/news/intel-lays-out-new-roadmap-for-ai-portfolio/}{Intel
  lays out new roadmap for {AI} portfolio}, Online Article (May 2018).
\newline\urlprefix\url{https://www.top500.org/news/intel-lays-out-new-roadmap-for-ai-portfolio/}

\bibitem{Strzodka2006}
R.~Strzodka, D.~Goddeke, Pipelined mixed precision algorithms on fpgas for fast
  and accurate pde solvers from low precision components, in: 14th Annual IEEE
  Symposium on Field-Programmable Custom Computing Machines, IEEE, 2006.
\newblock \href {https://doi.org/10.1109/FCCM.2006.57}
  {\path{doi:10.1109/FCCM.2006.57}}.

\bibitem{KenterVector}
T.~Kenter, G.~Vaz, C.~Plessl, Partitioning and vectorizing binary applications,
  in: Proc. Int. Conf. on Reconfigurable Computing: Architectures, Tools and
  Applications (ARC), Vol. 8405 of Lecture Notes in Computer Science, Springer,
  2014, pp. 144--155.
\newblock \href {https://doi.org/10.1007/978-3-319-05960-0\_13}
  {\path{doi:10.1007/978-3-319-05960-0\_13}}.

\bibitem{KenterPragma}
T.~Kenter, H.~Schmitz, C.~Plessl, Pragma based parallelization - trading
  hardware efficiency for ease of use?, in: 2012 International Conference on
  Reconfigurable Computing and FPGAs, IEEE, 2012.
\newblock \href {https://doi.org/10.1109/ReConFig.2012.6416773}
  {\path{doi:10.1109/ReConFig.2012.6416773}}.

\bibitem{Krajewski}
F.~R. Krajewski, M.~Parrinello, Linear scaling electronic structure
  calculations and accurate statistical mechanics sampling with noisy forces,
  Phys. Rev. B 73 (2006) 041105.
\newblock \href {https://doi.org/10.1103/PhysRevB.73.041105}
  {\path{doi:10.1103/PhysRevB.73.041105}}.

\bibitem{Richters}
D.~Richters, T.~D. K\"uhne, Self-consistent field theory based molecular
  dynamics with linear system-size scaling, J. Chem. Phys. 140~(13) (2014)
  134109.
\newblock \href {https://doi.org/10.1063/1.4869865}
  {\path{doi:10.1063/1.4869865}}.

\bibitem{Karhan}
K.~Karhan, R.~Z. Khaliullin, T.~D. K\"uhne, On the role of interfacial hydrogen
  bonds in ``on-water'' catalysis, J. Chem. Phys. 141~(22) (2014) 22D528.
\newblock \href {https://doi.org/10.1063/1.4902537}
  {\path{doi:10.1063/1.4902537}}.

\bibitem{cp2k}
J.~Hutter, M.~Iannuzzi, F.~Schiffmann, J.~VandeVondele, Cp2k: atomistic
  simulations of condensed matter systems, WIREs Comput. Mol. Sci. 4 (2014)
  15--25.
\newblock \href {https://doi.org/10.1002/wcms.1159}
  {\path{doi:10.1002/wcms.1159}}.

\bibitem{EIP1}
M.~Z. Bazant, E.~Kaxiras, Modeling of covalent bonding in solids by inversion
  of cohesive energy curves, Phys. Rev. Lett. 77 (1996) 4370--4373.
\newblock \href {https://doi.org/10.1103/PhysRevLett.77.4370}
  {\path{doi:10.1103/PhysRevLett.77.4370}}.

\bibitem{EIP2}
M.~Z. Bazant, E.~Kaxiras, J.~F. Justo, Environment-dependent interatomic
  potential for bulk silicon, Phys. Rev. B 56 (1997) 8542--8552.
\newblock \href {https://doi.org/10.1103/PhysRevB.56.8542}
  {\path{doi:10.1103/PhysRevB.56.8542}}.

\bibitem{Ricci}
A.~Ricci, G.~Ciccotti, Algorithms for brownian dynamics, Mol. Phys. 101 (2003)
  1927--1931.
\newblock \href {https://doi.org/10.1080/0026897031000108113}
  {\path{doi:10.1080/0026897031000108113}}.

\bibitem{Berendsen}
H.~J.~C. Berendsen, J.~P.~M. Postma, W.~F. van Gunsteren, A.~DiNola, J.~R.
  Haak, Molecular dynamics with coupling to an external bath, J. Chem. Phys.
  81~(8) (1984) 3684--3690.
\newblock \href {https://doi.org/10.1063/1.448118}
  {\path{doi:10.1063/1.448118}}.

\bibitem{TDKwater}
T.~D. K\"uhne, M.~Krack, M.~Parrinello, Static and dynamical properties of
  liquid water from first principles by a novel car-parrinello-like approach,
  J. Chem. Theory and Comput. 5~(2) (2009) 235--241.
\newblock \href {https://doi.org/10.1021/ct800417q}
  {\path{doi:10.1021/ct800417q}}.

\bibitem{TDKrev}
R.~Z. Khaliullin, T.~D. K\"uhne, Microscopic properties of liquid water from
  combined ab initio molecular dynamics and energy decomposition studies, Phys.
  Chem. Chem. Phys. 15~(38) (2013) 15746--15766.
\newblock \href {https://doi.org/10.1039/C3CP51039E}
  {\path{doi:10.1039/C3CP51039E}}.

\bibitem{RZK}
H.~Scheiber, Y.~Shi, R.~Z. Khaliullin, Communication: Compact orbitals enable
  low-cost linear-scaling ab initio molecular dynamics for weakly-interacting
  systems, J. Chem. Phys. 148~(23) (2018) 231103.
\newblock \href {https://doi.org/10.1063/1.5029939}
  {\path{doi:10.1063/1.5029939}}.

\bibitem{KAF}
K.~A.~F. R\"ohrig, T.~D. K\"uhne, Optimal calculation of the pair correlation
  function for an orthorhombic system, Phys. Rev. E 87 (2013) 045301.
\newblock \href {https://doi.org/10.1103/PhysRevE.87.045301}
  {\path{doi:10.1103/PhysRevE.87.045301}}.

\bibitem{iPi}
V.~Kapil, M.~Rossi, O.~Marsalek, R.~Petraglia, Y.~Litman, T.~Spura, B.~Cheng,
  A.~Cuzzocrea, R.~H. Mei{\ss}ner, D.~M. Wilkins, B.~A. Helfrecht, P.~Juda,
  S.~P. Bienvenue, W.~Fang, J.~Kessler, I.~Poltavsky, S.~Vandenbrande,
  J.~Wieme, C.~Corminboeuf, T.~D. K{\"u}hne, D.~E. Manolopoulos, T.~E.
  Markland, J.~O. Richardson, A.~Tkatchenko, G.~A. Tribello, V.~V. Speybroeck,
  M.~Ceriotti, {i-PI} 2.0: A universal force engine for advanced molecular
  simulations, Comp. Phys. Commun. 236 (2019) 214--223.
\newblock \href {https://doi.org/10.1016/j.cpc.2018.09.020}
  {\path{doi:10.1016/j.cpc.2018.09.020}}.

\bibitem{Steane}
A.~M. Steane, Efficient fault-tolerant quantum computing, Nature 399 (1999)
  124--126.
\newblock \href {https://doi.org/10.1038/20127} {\path{doi:10.1038/20127}}.

\bibitem{Knill}
E.~Knill, Quantum computing with realistically noisy devices, Nature 434 (2005)
  39--44.
\newblock \href {https://doi.org/10.1038/nature03350}
  {\path{doi:10.1038/nature03350}}.

\bibitem{Blatt}
J.~Benhelm, G.~Kirchmair, C.~F. Roos, R.~Blatt, Towards fault-tolerant quantum
  computing with trapped ions, Nature Physics 4 (2008) 463--466.
\newblock \href {https://doi.org/10.1038/nphys961}
  {\path{doi:10.1038/nphys961}}.

\bibitem{Chow}
J.~M. Chow, J.~M. Gambetta, E.~Magesan, D.~W. Abraham, A.~W. Cross, B.~R.
  Johnson, N.~A. Masluk, C.~A. Ryan, J.~A. Smolin, S.~J. Srinivasan,
  M.~Steffen, Implementing a strand of a scalable fault-tolerant quantum
  computing fabric, Nature Commun. 5 (2014) 4015.
\newblock \href {https://doi.org/10.1038/ncomms5015}
  {\path{doi:10.1038/ncomms5015}}.

\bibitem{White}
G.~{Efstathiou}, M.~{Davis}, C.~S. {Frenk}, S.~D.~M. {White}, {Numerical
  techniques for large cosmological N-body simulations}, The Astrophysical
  Journal 57 (1985) 241--260.
\newblock \href {https://doi.org/10.1086/191003} {\path{doi:10.1086/191003}}.

\bibitem{Makino}
L.~{Hernquist}, P.~{Hut}, J.~{Makino}, {Discreteness Noise versus Force Errors
  in N-Body Simulations}, The Astrophysical Journal 402 (1993) L85.
\newblock \href {https://doi.org/10.1086/186706} {\path{doi:10.1086/186706}}.

\bibitem{LassAC}
M.~Lass, S.~Mohr, H.~Wiebeler, T.~D. K\"{u}hne, C.~Plessl, A massively parallel
  algorithm for the approximate calculation of inverse p-th roots of large
  sparse matrices, in: Proc. Platform for Advanced Scientific Computing
  Conference, PASC '18, ACM, New York, NY, USA, 2018, pp. 7:1--7:11.
\newblock \href {https://doi.org/10.1145/3218176.3218231}
  {\path{doi:10.1145/3218176.3218231}}.

\end{thebibliography}

\end{document}
